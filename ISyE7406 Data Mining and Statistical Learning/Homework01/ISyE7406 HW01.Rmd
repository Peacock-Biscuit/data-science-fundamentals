---
title: "ISyE7406 Homework 1"
output:
  word_document: 
    keep_md: yes
  html_notebook: default
  pdf_document: 
    keep_tex: yes
  html_document:
    df_print: paged
---

# Problem 1

```{r Data}
data_7406 <- data.frame(Y = c(2, 1, 4, 2, 2, 5), 
                   A = c(1, 0, 1, 1, 0, 1), 
                   B = c(0, 1, 1, 0, 1, 1))
print(data_7406)
```
## (a)

There are 6 observations, and we can write the observed data in the matrix form Yn×1 = Xn×p βp×1 + εn×1 with n = 6 and p = 2. From this viewpoint, using the linear regression to

* (i) estimate the weights of balls A and B; and
* (ii) find a 70% confidence interval on the weight of ball A.
* (iii) Suppose the student plans to measure the weight of ball A one more time. Find a 70% prediction interval on the new observed weight of ball A.

### (i)

```{r Regression Model}
lmod <- lm(Y ~ 0 + A + B, data = data_7406)
summary(lmod)
```

### (ii)

```{r Confidence Interval}
qt(.85, 6 - 2)
2.3333 + c(-1, 1) * 1.189567 * 0.3727
```

### (iii)
```{r Matrix Split}
x <- data.matrix(data_7406[2:3])
```

```{r Prediction Interval}
qt(.85, 6 - 2)
x <- data.matrix(data_7406[2:3])
# transpose x * x to 
diagonal <- t(x) %*% x
print(solve(diagonal))
#sigma_square: RSS/4
print(anova(lmod))
RSS <- 1.667
sigma_hat <- sqrt(RSS / 4)
#new data
x_new <- c(1, 0)
x_new_square <- t(x_new) %*% x_new

#prediction interval
2.3333 + c(-1, 1) * 1.189567 * sigma_hat * sqrt(1 + 0.3333333 * 1)
```

## (b)
Repeat part (a) by writing the “new observed average weights” in the matrix form Yn×1 = Xn×p βp×1 + εn×1 with n=3 and p=2.

```{r New Data}
data_new <- data.frame(Y_new = c(2, 1.5, 4.5), 
                   A_new = c(1, 0, 1), 
                   B_new = c(0, 1, 1))
print(data_new)
```

### (i)

```{r New Regression Model}
lmod_new <- lm(Y_new ~ 0 + A_new + B_new, data = data_new)
summary(lmod_new)
```

### (ii)

```{r New Confidence Interval}
qt(.85, 3 - 2)
2.3333 + c(-1, 1) * 1.962611 * 0.4714
```

### (iii)

```{r New Prediction Interval}
qt(.85, 3 - 2)
x_b <- data.matrix(data_new[2:3])
# transpose x * x to 
diagonal_b <- t(x_b) %*% x_b
print(solve(diagonal_b))
#sigma_square: RSS/4
print(anova(lmod_new))
RSS_b <- 0.6666667
sigma_hat_b <- sqrt(RSS_b / (3 - 2))
#new data 
x_new_b <- c(1, 0)
x_new_square_b <- t(x_new_b) %*% x_new_b

#prediction interval
2.3333 + c(-1, 1) * 1.962611 * sigma_hat_b * sqrt(1 + 0.6666667 * 1)
```

## (c) Compare your results in (a) and (b)


# Problem 3 (R exercise)
Consider the zipcode data, which are available from the book website: <www- stat.stanford.edu/ElemStatLearn>. 
In the zipcode data, the first column stands for the response (Y) and the other columns stand for the independent variables (Xi’s). The detailed description can be found from [http://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/zip.info.txt]

Here we consider only the classification problem between 2’s and 7’s.

## (a) Read Data
 Let us first obtain the training data. The following R code can yield the desired training data named as “ziptrain27”

```{r Read Data}
ziptrain <- read.table(file="http://www.isye.gatech.edu/~ymei/7406/Handouts/zip.train.csv", sep = ",")
ziptrain27 <- subset(ziptrain, ziptrain[,1]==2 | ziptrain[,1]==7)
ziptrain27
```

## (b) Exploratory Data Analysis

```{r}
## To see the letter picture of the 5-th row by changing the row observation to a matrix rowindex = 5; 
## You can try other "rowindex" values to see other rows ziptrain27[rowindex,1];
for (i in seq(1:25)) {
  Xval = t(matrix(data.matrix(ziptrain27[,-1])[rowindex = i,], byrow = TRUE, 16, 16)[16:1,]); 
image(Xval,col=gray(0:32/32),axes=FALSE) 
}

## Also try "col=gray(0:32/32)"
```

```{r}
dim(ziptrain27)
sum(ziptrain27[,1] == 2); sum(ziptrain27[,1] == 7)
summary(ziptrain27)
ziptrain27Cor <- round(cor(ziptrain27),2)
```
```{r}
library(ggplot2)
library(reshape2)
ziptrain27Cor_ <- melt(ziptrain27Cor)
head(ziptrain27Cor_)
ggplot(data = ziptrain27Cor_, aes(x=Var1, y=Var2, fill=value)) +  geom_tile()
```

## (c) Using the training data “ziptrain27” to build the classification rule by (i) linear regression; and (ii) the KNN with k = 1, 3, 5, 7 and 15. Find the training errors of each choice.

```{r linear Regression}
mod1 <- lm( V1 ~ . , data= ziptrain27);
pred1.train <- predict.lm(mod1, ziptrain27[,-1]);
y1pred.train <- 2 + 5*(pred1.train >= 4.5);
mean( y1pred.train != ziptrain27[,1])
```

```{r KNN}
library(class);
xnew <- ziptrain27[,-1];
z <- c()
for (i in c(1, 3, 5, 7, 15)) {
  ypred2.train <- knn(ziptrain27[,-1], xnew, cl = ziptrain27[,1], k = i); 
  z <- c(z, mean(ypred2.train != ziptrain27[,1]))
}
z
```

## (4) Let us consider the testing data set, and derive the testing errors of each classification rule in (3).

```{r}
ziptest <- read.table(file="http://www.isye.gatech.edu/~ymei/7406/Handouts/zip.test.csv", sep = ",");
ziptest27 <- subset(ziptest, ziptest[,1]==2 | ziptest[,1]==7);
## Testing error of KNN
kk <- 1;
xnew2 <- ziptest27[,-1];
new <- c()
for (i in c(1, 3, 5, 7, 15)) {
  ypred2.test <- knn(ziptrain27[,-1], xnew2, cl = ziptrain27[,1], k = i); 
  new <- c(new, mean(ypred2.train != ziptrain27[,1]))
}
new
```

```{r Line Chart}
plot(z,type = "o",col = "red", xlab = "K class", ylab = "Error Rate", 
   main = "Training Errors VS. Testing Errors", xaxt = "n", las = 1)
lines(new, type = "o", col = "blue")

axis(1, at=1:5, labels=c("1","3","5","7","15"))
legend("bottomright", 
  legend = c("Testing Errors", "Training Errors"), 
  col = c("Blue", "Red"), 
  pch = c(1,1),
  bty = "n", 
  pt.cex = 2, 
  cex = 1.2, 
  text.col = "black", 
  horiz = F , 
  inset = c(0.1, 0.1))
```

